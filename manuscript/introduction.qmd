# Background {#sec-introduction}

Quantitative research relies on the availability of high-quality and complete data. Yet, in practice, datasets are often characterized by incomplete cases, leading to possibly biased estimates and reduced statistical power. Multiple imputation (MI) has become a widely used solution for addressing this issue. MI involves replacing an incomplete dataset with multiple imputed datasets and analyzing each separately, with the final estimates being pooled using Rubin's rules [@Rubin1987]. However, MI algorithms that fail to capture complex data patterns can also lead to biases [@Seaman2012; @Curnow2023]. This issue is particularly pronounced for multivariate missingness and when using fully conditional specification (FCS) MI algorithms, such as those implemented in the popular `mice` ("Multivariate Imputation by Chained Equations") `R` package [@mice]. FCS MI operates by using a series of univariate imputation models in a cyclic fashion, each of which requires careful specification of non-linear relationships and interactions, which can be impractical. Therefore, developing procedures that can automatically address these complexities is of significant interest.

With advancements in computational power, sophisticated algorithms have become essential for handling complex data structures with the aim to improve missing data imputation. Techniques like random forest (RF) [@Breiman2001] and classification and regression trees (CART) [@Breiman2017] have been integrated into popular imputation packages like `mice`. These methods have demonstrated better preservation of complex data patterns compared to traditional approaches [@Burgette2010; @Shah2014; @Doove2014]. More recently, @Deng2023 implemented an MI algorithm using the XGBoost technique [@Chen2016] within the `mixgb` `R` package, which outperformed RF and CART-based imputations. Briefly, XGBoost is a fast, tree-based boosting algorithm that builds a strong predictive model by combining multiple weak learners in a sequential manner.

Despite their benefits, models like CART and RF often require larger sample sizes to reliably learn the underlying data structure due to their flexibility in modeling interactions and categorizing continuous variables [@Riley2020].
While large datasets are sometimes available, such as in observational studies, randomized controlled trials (RCTs) or other quasi-experimental designs often face sample size limitations due to high costs of data collection.
Nevertheless, addressing missing data remains crucial to maintaining the credibility of trial conclusions, as emphasized by @Little2012. Studies such as those by @Spekreijse2023 and @Strandell2024 provide examples where MI with predictive mean matching (PMM) was applied to handle missing data in primary analyses.
Classical regression-based methods, such as generalized additive models (GAM) [@Wood2017], offer a solution to capture potential non-linearities more effectively.
GAM extensions, such as generalized additive models for location, scale, and shape (GAMLSS) [@Stasinopoulos2007], have been explored for MI and are available via the `mice` add-on package `imputerobust` [@ImputeRobust]. However, these methods can be computationally intensive when many predictors are involved.

An alternative approach proposed in this paper is multivariate adaptive regression splines (MARS), introduced by @Friedman1991. MARS balances computational efficiency and flexibility, modeling non-linearities and interactions without requiring large datasets as in tree-based models, which may suffer from a lack of smoothness [@Hastie2017]. Although the use of MARS for imputation tasks is not entirely new [@Sanchez2011; @Sanchezlasheras2020], its application in a FCS MI framework remains unexplored.

This paper aims to implement an FCS MI algorithm based on MARS and evaluate its performance against existing models within the `mice` framework. 
[Section -@sec-mars] briefly introduces the MARS algorithm, and [Section -@sec-simulation] describes the simulation study, including the experimental settings, comparative models, and performance metrics.
[Section -@sec-results] presents the results of the simulation, followed by a discussion of the findings in [Section -@sec-discussion]. The paper concludes with a summary and future directions.
